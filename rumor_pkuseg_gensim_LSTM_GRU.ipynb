{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# é­”æ³•å‘½ä»¤ï¼Œä½¿ç”¨åç”»å›¾ä¸ç”¨showäº†\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# å¼•å…¥æ­£åˆ™\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.è§£å‹è¯å‘é‡å¹¶åŠ è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1è§£å‹è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# ç”¨æ¥è§£å‹æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings/sgns.weibo.bigram\", 'wb') as new_file, open(\"./embeddings/sgns.weibo.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2åŠ è½½è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensimç”¨æ¥åŠ è½½é¢„è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.weibo.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.è¯­æ–™é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1è¯»å–åŸå§‹æ–‡æœ¬\n",
    "* weiboï¼šDataFrameå­˜å‚¨çš„åšæ–‡åŠå…¶å¯¹åº”æ ‡ç­¾\n",
    "* contentï¼šlistå­˜å‚¨çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "* labelï¼šæ ‡ç­¾ï¼Œ1ä¸ºéè°£è¨€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013å¹´3æœˆ5æ—¥ï¼Œã€Šæ˜æŠ¥ã€‹åšäº†é¢˜ç›®ä¸ºã€Šå¦‚æœæœ‰æ¥ç”Ÿï¼Œä½ æ„¿ä¸æ„¿æ„å†åšä¸­å›½äººï¼Ÿã€‹çš„æŠ•ç¥¨è°ƒæŸ¥ï¼Œæˆªæ­¢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>å„ä½åŒå­¦ï¼Œæœ‰è®¤è¯†ä½åœ¨åæ˜Œè·¯é è¿‘ä¸­å…´è·¯çš„æµ·ä¼¦æ–°è‹‘çš„æœ‹å‹å—ï¼Ÿè½¬ç»™ä»–ä»¬çœ‹ä¸€ä¸‹å¥½ä¼â€¦â€¦å¸®åœ¨ä¸‹ç•™æ„ä¸€ä¸‹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>#åŒ—äº¬æ›å…‰# @å€å„¿å°çˆ½ ï¼šä»Šå¤©å›å®¶è·¯ä¸Šæ¥åˆ°è€å¦ˆç”µè¯ï¼Œè€å¦ˆå†’é›¨ç»™æˆ‘é€é¥­ï¼Œæ‹¿åˆ°é¥­æˆ‘è®©è€å¦ˆæ‰“è½¦å›...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ä¸€ä½å¯æ€œçš„94å²è€å¥¶å¥¶ï¼Œè€ä¼´å»ä¸–ï¼Œå„¿å­ä¼¤å¯’æ­»äº†ï¼Œä¸¤ä¸ªå­™å­åœ¨å¤–é¢æ‰“å·¥ï¼Œå¥¹æ¯å¤©ä¾é æ¡åƒåœ¾ä¸ºç”Ÿï¼Œæ¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>æœ‰äººå¥½å¥‡é»‘äººå¦¹å­æ˜¯å¦‚ä½•åŒ–å¦†çš„ä¹ˆï¼ŸLZå¸¦å›¾è¯¦è§£ [å“ˆå“ˆ] å¤ªæç¬‘äº†ï¼Œä¸€ç›´ä»¥ä¸ºä»–ä»¬æ˜¯ä¸åŒ–å¦†çš„ï½ï½ï¼ˆè½¬ï¼‰</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  2013å¹´3æœˆ5æ—¥ï¼Œã€Šæ˜æŠ¥ã€‹åšäº†é¢˜ç›®ä¸ºã€Šå¦‚æœæœ‰æ¥ç”Ÿï¼Œä½ æ„¿ä¸æ„¿æ„å†åšä¸­å›½äººï¼Ÿã€‹çš„æŠ•ç¥¨è°ƒæŸ¥ï¼Œæˆªæ­¢...\n",
       "1             1  å„ä½åŒå­¦ï¼Œæœ‰è®¤è¯†ä½åœ¨åæ˜Œè·¯é è¿‘ä¸­å…´è·¯çš„æµ·ä¼¦æ–°è‹‘çš„æœ‹å‹å—ï¼Ÿè½¬ç»™ä»–ä»¬çœ‹ä¸€ä¸‹å¥½ä¼â€¦â€¦å¸®åœ¨ä¸‹ç•™æ„ä¸€ä¸‹...\n",
       "2             1  #åŒ—äº¬æ›å…‰# @å€å„¿å°çˆ½ ï¼šä»Šå¤©å›å®¶è·¯ä¸Šæ¥åˆ°è€å¦ˆç”µè¯ï¼Œè€å¦ˆå†’é›¨ç»™æˆ‘é€é¥­ï¼Œæ‹¿åˆ°é¥­æˆ‘è®©è€å¦ˆæ‰“è½¦å›...\n",
       "3             0  ä¸€ä½å¯æ€œçš„94å²è€å¥¶å¥¶ï¼Œè€ä¼´å»ä¸–ï¼Œå„¿å­ä¼¤å¯’æ­»äº†ï¼Œä¸¤ä¸ªå­™å­åœ¨å¤–é¢æ‰“å·¥ï¼Œå¥¹æ¯å¤©ä¾é æ¡åƒåœ¾ä¸ºç”Ÿï¼Œæ¯...\n",
       "4             1  æœ‰äººå¥½å¥‡é»‘äººå¦¹å­æ˜¯å¦‚ä½•åŒ–å¦†çš„ä¹ˆï¼ŸLZå¸¦å›¾è¯¦è§£ [å“ˆå“ˆ] å¤ªæç¬‘äº†ï¼Œä¸€ç›´ä»¥ä¸ºä»–ä»¬æ˜¯ä¸åŒ–å¦†çš„ï½ï½ï¼ˆè½¬ï¼‰"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo = weibo.dropna()#åˆ é™¤ç¼ºå¤±å€¼\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†DataFrameä¸­çš„Seriesè½¬æ¢ä¸ºlist\n",
    "content = weibo.content.values.tolist()\n",
    "label=weibo.is_not_rumor.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä¸€ä½å¯æ€œçš„94å²è€å¥¶å¥¶ï¼Œè€ä¼´å»ä¸–ï¼Œå„¿å­ä¼¤å¯’æ­»äº†ï¼Œä¸¤ä¸ªå­™å­åœ¨å¤–é¢æ‰“å·¥ï¼Œå¥¹æ¯å¤©ä¾é æ¡åƒåœ¾ä¸ºç”Ÿï¼Œæ¯å¤©æ¡åƒåœ¾æ¡åˆ°å‡Œæ™¨2ç‚¹ï¼Œä¹Ÿåªèƒ½è½¬5ã€6å…ƒã€‚å¥¶å¥¶åŒè…¿å·²ç»è£‚å¼€ï¼Œå› ä¸ºæ²¡é’±ï¼Œä¸€ç›´æ²¡æœ‰åŒ»æ²»ã€‚æ±‚æ‰©æ•£~~æ¯è½¬ä¸€æ¡å¾®åšï¼Œè…¾è®¯å…¬ç›Šå°±åƒè€å¥¶å¥¶æå‡º1æ¯›é’±ï¼Œå¤šè½¬å‡ æ¬¡å§ï¼Œä¸ä¼šè„äº†ä½ å¾®åšï¼Œå¯¹ä¹ˆï¼Ÿ', 'æœ‰äººå¥½å¥‡é»‘äººå¦¹å­æ˜¯å¦‚ä½•åŒ–å¦†çš„ä¹ˆï¼ŸLZå¸¦å›¾è¯¦è§£ [å“ˆå“ˆ] å¤ªæç¬‘äº†ï¼Œä¸€ç›´ä»¥ä¸ºä»–ä»¬æ˜¯ä¸åŒ–å¦†çš„ï½ï½ï¼ˆè½¬ï¼‰']\n"
     ]
    }
   ],
   "source": [
    "print (content[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2è¿›è¡Œåˆ†è¯å’Œtokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lancopku/PKUSeg-python\n",
    "\n",
    "å¯¹æ¯ä¸€æ¡å¾®åšæ–‡æœ¬textï¼Œ\n",
    "1. å»æ‰æ¯ä¸ªæ ·æœ¬çš„æ ‡ç‚¹ç¬¦å·ï¼›\n",
    "2. ç”¨pkusegåˆ†è¯ï¼Œå¾—åˆ°å­˜æ”¾åˆ†è¯ç»“æœçš„cut_listï¼›\n",
    "3. å»æ‰cut_listä¸­çš„åœç”¨è¯å¾—åˆ°cut_list_cleanï¼›\n",
    "3. å°†åˆ†è¯ç»“æœcut_list_cleanç´¢å¼•åŒ–ï¼ˆä½¿ç”¨åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ä¸ä¸­å›½äººæ°‘å¤§å­¦ DBIIR å®éªŒå®¤çš„ç ”ç©¶è€…å¼€æºçš„\"chinese-word-vectors\"ï¼‰ï¼Œè¿™æ ·æ¯ä¸€ä¾‹è¯„ä»·çš„æ–‡æœ¬å˜æˆä¸€æ®µç´¢å¼•æ•°å­—ï¼Œå¯¹åº”ç€é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­çš„è¯ã€‚\n",
    "\n",
    "å°†æ¯ä¸ªtextçš„ç»“æœå­˜åˆ°train_tokensä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥åœç”¨è¯\n",
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords.stopword.values.tolist()#è½¬ä¸ºlistå½¢å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lancopku/pkuseg-python/releases/download/v0.0.16/web.zip\" to C:\\Users\\dell/.pkuseg\\web.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17478354/17478354 [00:12<00:00, 1432747.33it/s]\n"
     ]
    }
   ],
   "source": [
    "seg = pkuseg.pkuseg(model_name='web')  # ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½æ‰€å¯¹åº”çš„ç»†é¢†åŸŸæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for text in content:\n",
    "    # å»æ‰æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # pkusegåˆ†è¯\n",
    "    cut_list = seg.cut(text)\n",
    "\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_list_clean=[]\n",
    "    for word in cut_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_list_clean.append(word)\n",
    "    \n",
    "    #ç´¢å¼•åŒ–\n",
    "    for i, word in enumerate(cut_list_clean): # enumerate()\n",
    "        try:\n",
    "            # å°†è¯è½¬æ¢ä¸ºç´¢å¼•index\n",
    "            cut_list_clean[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # å¦‚æœè¯ä¸åœ¨å­—å…¸ä¸­ï¼Œåˆ™è¾“å‡º0\n",
    "            cut_list_clean[i] = 0\n",
    "    train_tokens.append(cut_list_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3ç´¢å¼•é•¿åº¦æ ‡å‡†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºæ¯æ®µè¯„è¯­çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œå¦‚æœå•çº¯å–æœ€é•¿çš„ä¸€ä¸ªè¯„è¯­ï¼Œå¹¶æŠŠå…¶ä»–è¯„å¡«å……æˆåŒæ ·çš„é•¿åº¦ï¼Œè¿™æ ·ååˆ†æµªè´¹è®¡ç®—èµ„æºï¼Œæ‰€ä»¥å–ä¸€ä¸ªæŠ˜è¡·çš„é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è·å¾—æ‰€æœ‰tokensçš„é•¿åº¦\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# å–tokenså¹³å‡å€¼å¹¶åŠ ä¸Šä¸¤ä¸ªtokensçš„æ ‡å‡†å·®ï¼Œ\n",
    "# å‡è®¾tokensé•¿åº¦çš„åˆ†å¸ƒä¸ºæ­£æ€åˆ†å¸ƒï¼Œåˆ™max_tokensè¿™ä¸ªå€¼å¯ä»¥æ¶µç›–95%å·¦å³çš„æ ·æœ¬\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paddingï¼ˆå¡«å……ï¼‰å’Œtruncatingï¼ˆä¿®å‰ªï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æŠŠæ–‡æœ¬è½¬æ¢ä¸ºtokensï¼ˆç´¢å¼•ï¼‰ä¹‹åï¼Œæ¯ä¸€ä¸²ç´¢å¼•çš„é•¿åº¦å¹¶ä¸ç›¸ç­‰ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿æ¨¡å‹çš„è®­ç»ƒæˆ‘ä»¬éœ€è¦æŠŠç´¢å¼•çš„é•¿åº¦æ ‡å‡†åŒ–ï¼Œä¸Šé¢æˆ‘ä»¬é€‰æ‹©äº†max_tokensä¸ªå¯ä»¥æ¶µç›–95%è®­ç»ƒæ ·æœ¬çš„é•¿åº¦ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¿›è¡Œpaddingå’Œtruncatingï¼Œæˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨'pre'çš„æ–¹æ³•ï¼Œè¿™ä¼šåœ¨æ–‡æœ¬ç´¢å¼•çš„å‰é¢å¡«å……0ï¼Œå› ä¸ºæ ¹æ®ä¸€äº›ç ”ç©¶èµ„æ–™ä¸­çš„å®è·µï¼Œå¦‚æœåœ¨æ–‡æœ¬ç´¢å¼•åé¢å¡«å……0çš„è¯ï¼Œä¼šå¯¹æ¨¡å‹é€ æˆä¸€äº›ä¸è‰¯å½±å“ã€‚ \n",
    "\n",
    "è¿›è¡Œpaddingå’Œtruncatingï¼Œ è¾“å…¥çš„train_tokensæ˜¯ä¸€ä¸ªlist\n",
    "è¿”å›çš„train_padæ˜¯ä¸€ä¸ªnumpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4å‡†å¤‡Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œéœ€è¦å‡†å¤‡ä¸€ä¸ªç»´åº¦ä¸º (ğ‘›ğ‘¢ğ‘šğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š) çš„embeddingçŸ©é˜µï¼Œnum wordsä»£è¡¨ä½¿ç”¨çš„è¯æ±‡çš„æ•°é‡ã€‚\n",
    "\n",
    "\n",
    "* ä¸è¿›è¡Œè¯å‘é‡çš„è®­ç»ƒï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡â€”â€”åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ä¸ä¸­å›½äººæ°‘å¤§å­¦ DBIIR å®éªŒå®¤çš„ç ”ç©¶è€…å¼€æºçš„\"chinese-word-vectors\"ï¼›https://github.com/Embedding/Chinese-Word-Vectors ï¼›emdedding dimensionåœ¨ç°åœ¨ä½¿ç”¨çš„é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­æ˜¯300ï¼Œæ¯ä¸€ä¸ªè¯æ±‡éƒ½ç”¨ä¸€ä¸ªé•¿åº¦ä¸º300çš„å‘é‡è¡¨ç¤ºã€‚\n",
    "\n",
    "\n",
    "* æ³¨æ„åªé€‰æ‹©ä½¿ç”¨å‰50kä¸ªä½¿ç”¨é¢‘ç‡æœ€é«˜çš„è¯ï¼Œåœ¨è¿™ä¸ªé¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­ï¼Œä¸€å…±æœ‰260ä¸‡è¯æ±‡é‡ï¼Œå¦‚æœå…¨éƒ¨ä½¿ç”¨åœ¨åˆ†ç±»é—®é¢˜ä¸Šä¼šå¾ˆæµªè´¹è®¡ç®—èµ„æºï¼Œå› ä¸ºè®­ç»ƒæ ·æœ¬å¾ˆå°ï¼Œå¦‚æœæœ‰æ›´å¤šçš„è®­ç»ƒæ ·æœ¬æ—¶ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸Šå¯ä»¥è€ƒè™‘å‡å°‘ä½¿ç”¨çš„è¯æ±‡é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# åˆå§‹åŒ–embedding_matrixï¼Œä¹‹ååœ¨kerasä¸Šè¿›è¡Œåº”ç”¨\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrixä¸ºä¸€ä¸ª [num_wordsï¼Œembedding_dim] çš„çŸ©é˜µ\n",
    "# ç»´åº¦ä¸º 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]#å‰50000ä¸ªindexå¯¹åº”çš„è¯çš„è¯å‘é‡\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# æ£€æŸ¥indexæ˜¯å¦å¯¹åº”ï¼Œ\n",
    "# è¾“å‡º300æ„ä¹‰ä¸ºé•¿åº¦ä¸º300çš„embeddingå‘é‡ä¸€ä¸€å¯¹åº”\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‡ºäº”ä¸‡ä¸ªè¯å‘é‡çš„è¯ç”¨0ä»£æ›¿\n",
    "train_pad[train_pad>=num_words ] = 0\n",
    "\n",
    "# å‡†å¤‡targetå‘é‡ï¼Œå‰2000æ ·æœ¬ä¸º1ï¼Œå2000ä¸º0\n",
    "train_target = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.è®­ç»ƒè¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬çš„åˆ†å‰²\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%çš„æ ·æœ¬ç”¨æ¥è®­ç»ƒï¼Œå‰©ä½™10%ç”¨æ¥æµ‹è¯•\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2æ­å»ºç½‘ç»œç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ç”¨kerasæ­å»ºLSTMæ¨¡å‹ï¼Œæ¨¡å‹çš„ç¬¬ä¸€å±‚æ˜¯Embeddingå±‚ï¼Œåªæœ‰å½“æˆ‘ä»¬æŠŠtokensç´¢å¼•è½¬æ¢ä¸ºè¯å‘é‡çŸ©é˜µä¹‹åï¼Œæ‰å¯ä»¥ç”¨ç¥ç»ç½‘ç»œå¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†ã€‚ kerasæä¾›äº†Embeddingæ¥å£ï¼Œé¿å…äº†ç¹ççš„ç¨€ç–çŸ©é˜µæ“ä½œã€‚\n",
    "* åœ¨Embeddingå±‚æˆ‘ä»¬è¾“å…¥çš„çŸ©é˜µä¸ºï¼š(ğ‘ğ‘ğ‘¡ğ‘â„ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘šğ‘ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ )è¾“å‡ºçŸ©é˜µä¸º:(ğ‘ğ‘ğ‘¡ğ‘â„ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘šğ‘ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š)ã€‚\n",
    "* ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œå°†trainableè®¾ä¸ºFalseï¼Œå³ä¸å¯è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================model1=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model1.add(Bidirectional(GRU(32)))\n",
    "model1.add(Dense(6, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3æ¨¡å‹é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹ä¿å­˜ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰ã€early stopingã€å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path=\"./checkpoint/rumor_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model.load_weights(checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_save_path, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰early stopingå¦‚æœ3ä¸ªepochå†…validation lossæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# è‡ªåŠ¨é™ä½learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# å®šä¹‰callbackå‡½æ•°\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model1==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path1=\"./checkpoint/rumor_GRU.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path1+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model1.load_weights(checkpoint_save_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint1 = ModelCheckpoint(filepath=checkpoint_save_path1, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2743 samples, validate on 305 samples\n",
      "Epoch 1/20\n",
      "2743/2743 [==============================] - 69s 25ms/sample - loss: 0.6390 - accuracy: 0.6482 - val_loss: 0.4600 - val_accuracy: 0.8295\n",
      "Epoch 2/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.4470 - accuracy: 0.8039\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2743/2743 [==============================] - 14s 5ms/sample - loss: 0.4463 - accuracy: 0.8046 - val_loss: 0.4779 - val_accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.4171 - accuracy: 0.8181 - val_loss: 0.3556 - val_accuracy: 0.8426\n",
      "Epoch 4/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3767 - accuracy: 0.8400 - val_loss: 0.3397 - val_accuracy: 0.8623\n",
      "Epoch 5/20\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.3627 - accuracy: 0.8498 - val_loss: 0.3333 - val_accuracy: 0.8590\n",
      "Epoch 6/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3522 - accuracy: 0.8600 - val_loss: 0.3276 - val_accuracy: 0.8721\n",
      "Epoch 7/20\n",
      "2743/2743 [==============================] - 12s 5ms/sample - loss: 0.3420 - accuracy: 0.8618 - val_loss: 0.3238 - val_accuracy: 0.8689\n",
      "Epoch 8/20\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.3340 - accuracy: 0.8647 - val_loss: 0.3211 - val_accuracy: 0.8754\n",
      "Epoch 9/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3233 - accuracy: 0.8724 - val_loss: 0.3171 - val_accuracy: 0.8721\n",
      "Epoch 10/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3132 - accuracy: 0.8746 - val_loss: 0.3117 - val_accuracy: 0.8787\n",
      "Epoch 11/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8802\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3020 - accuracy: 0.8801 - val_loss: 0.3146 - val_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2927 - accuracy: 0.8859 - val_loss: 0.3076 - val_accuracy: 0.8852\n",
      "Epoch 13/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8884\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.2895 - accuracy: 0.8881 - val_loss: 0.3089 - val_accuracy: 0.8787\n",
      "Epoch 14/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8873\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2888 - accuracy: 0.8877 - val_loss: 0.3088 - val_accuracy: 0.8787\n",
      "Epoch 15/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8865\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2886 - accuracy: 0.8877 - val_loss: 0.3087 - val_accuracy: 0.8787\n",
      "Epoch 16/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.8888\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2886 - accuracy: 0.8877 - val_loss: 0.3087 - val_accuracy: 0.8787\n",
      "Epoch 17/20\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.2886 - accuracy: 0.8877 - val_loss: 0.3087 - val_accuracy: 0.8787\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xcba3c901c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2743 samples, validate on 305 samples\n",
      "Epoch 1/20\n",
      "2743/2743 [==============================] - 29s 11ms/sample - loss: 0.7042 - accuracy: 0.5035 - val_loss: 0.6928 - val_accuracy: 0.5279\n",
      "Epoch 2/20\n",
      "2743/2743 [==============================] - 7s 2ms/sample - loss: 0.6920 - accuracy: 0.5472 - val_loss: 0.6910 - val_accuracy: 0.5279\n",
      "Epoch 3/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.6785 - accuracy: 0.5472 - val_loss: 0.6643 - val_accuracy: 0.5279\n",
      "Epoch 4/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.6348 - accuracy: 0.5472 - val_loss: 0.6109 - val_accuracy: 0.5279\n",
      "Epoch 5/20\n",
      "2743/2743 [==============================] - 4s 2ms/sample - loss: 0.5728 - accuracy: 0.5633 - val_loss: 0.5394 - val_accuracy: 0.7475\n",
      "Epoch 6/20\n",
      "2743/2743 [==============================] - 5s 2ms/sample - loss: 0.4963 - accuracy: 0.8064 - val_loss: 0.4369 - val_accuracy: 0.8459\n",
      "Epoch 7/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.4138 - accuracy: 0.8483 - val_loss: 0.3653 - val_accuracy: 0.8459\n",
      "Epoch 8/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.3249 - accuracy: 0.8684 - val_loss: 0.3333 - val_accuracy: 0.8689\n",
      "Epoch 9/20\n",
      "2743/2743 [==============================] - 4s 2ms/sample - loss: 0.2782 - accuracy: 0.8917 - val_loss: 0.3098 - val_accuracy: 0.8787\n",
      "Epoch 10/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9129\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2743/2743 [==============================] - 5s 2ms/sample - loss: 0.2404 - accuracy: 0.9129 - val_loss: 0.3311 - val_accuracy: 0.8656\n",
      "Epoch 11/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.2109 - accuracy: 0.9282 - val_loss: 0.3019 - val_accuracy: 0.8820\n",
      "Epoch 12/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.2021 - accuracy: 0.9304 - val_loss: 0.3012 - val_accuracy: 0.8787\n",
      "Epoch 13/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9330\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2743/2743 [==============================] - 3s 1ms/sample - loss: 0.1973 - accuracy: 0.9340 - val_loss: 0.3019 - val_accuracy: 0.8787\n",
      "Epoch 14/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9334 ETA: 0s - loss: 0.1929 - accuracy: \n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1941 - accuracy: 0.9336 - val_loss: 0.3018 - val_accuracy: 0.8787\n",
      "Epoch 15/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9330\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1937 - accuracy: 0.9340 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 16/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9334\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1936 - accuracy: 0.9340 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 17/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9338\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1936 - accuracy: 0.9340 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xcbb67b17c8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5åº”ç”¨äºæµ‹è¯•é›†\n",
    "é¦–å…ˆå¯¹æµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°äº†è¿˜ç®—æ»¡æ„çš„å‡†ç¡®åº¦ã€‚ä¹‹åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ï¼Œæ¥é¢„æµ‹è¾“å…¥çš„æ–‡æœ¬çš„ææ€§ï¼Œå¯è§æ¨¡å‹å¯¹äºå¦å®šå¥å’Œä¸€äº›ç®€å•çš„é€»è¾‘ç»“æ„éƒ½å¯ä»¥è¿›è¡Œå‡†ç¡®çš„åˆ¤æ–­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 2s 5ms/sample - loss: 0.3997 - accuracy: 0.8289\n",
      "Accuracy:82.89%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 1s 3ms/sample - loss: 0.3591 - accuracy: 0.8614\n",
      "Accuracy:86.14%\n"
     ]
    }
   ],
   "source": [
    "result = model1.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6å®ä¾‹å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rumor_LSTM(text,label):\n",
    "    print(text)\n",
    "    # å»æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # åˆ†è¯\n",
    "    cut = seg.cut(text)\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # é¢„æµ‹\n",
    "    dic={0:'è°£è¨€',1:'éè°£è¨€'}\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯éè°£è¨€','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯è°£è¨€','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…´ä»å¿ä»Šå¤©æŠ¢å°å­©æ²¡æŠ¢èµ°ï¼ŒæŠŠå­©å­æ¯äº²æ…äº†ä¸€åˆ€ï¼Œçœ‹è§è¿™è½¦çš„æ³¨æ„äº†ï¼ŒçœŸäº‹ï¼Œè½¦ç‰Œå·è¾½HFM055ï¼ï¼ï¼ï¼ï¼èµ¶ç´§æ•£æ’­ï¼ éƒ½åˆ«å¸¦å­©å­å‡ºå»çè½¬æ‚ äº† å°¤å…¶åˆ«è®©è€äººè‡ªå·±å¸¦å­©å­å‡ºå» å¤ªå±é™©äº† æ³¨æ„äº†ï¼ï¼ï¼ï¼è¾½HFM055åŒ—äº¬ç°ä»£æœ—åŠ¨ï¼Œåœ¨å„å­¦æ ¡é—¨å£æŠ¢å°å­©ï¼ï¼ï¼110å·²ç» è¯å®ï¼ï¼å…¨å¸‚é€šç¼‰ï¼ï¼\n",
      "çœŸå®æ˜¯è°£è¨€ é¢„æµ‹æ˜¯è°£è¨€ output=0.10\n",
      "---------------------------------------------\n",
      "é‡åº†çœŸå®æ–°é—»:2016å¹´6æœˆ1æ—¥åœ¨é‡åº†æ¢å¹³å¿è¢é©¿é•‡å‘ç”Ÿä¸€èµ·æŠ¢å„¿ç«¥äº‹ä»¶ï¼Œåšæ¡ˆäººä¸‰ä¸ªä¸­å¹´ç”·äººï¼Œåœ¨ä¸‰ä¸­å­¦æ ¡åˆ°é•‡è¡—ä¸Šçš„ä¸€æ¡å°è·¯ä¸Šï¼ŒæŠŠå°å­©ç›´æ¥å¼„æ™•(å„¿ç«¥æ˜¯è¢é©¿æ–°å¹¼å„¿å›­ä¸­ç­çš„ä¸€åå­¦ç”Ÿ)ï¼Œæ­£å‡†å¤‡å¸¦èµ°æ—¶è¢«å®¶é•¿åŠæ—¶å‘ç°ç”¨æ£’å­èµ¶èµ°äº†åšæ¡ˆäººï¼Œæ•…æ­¤è·æ•‘ï¼è¯·å„ä½åŒèƒä»¬ä»¥æ­¤å¼•èµ·éå¸¸é‡è§†ï¼Œå¸Œæœ›å¤§å®¶æœ‰çˆ±å¿ƒçš„äººä¼ é€’ä¸‹\n",
      "çœŸå®æ˜¯è°£è¨€ é¢„æµ‹æ˜¯è°£è¨€ output=0.10\n",
      "---------------------------------------------\n",
      "@å°¾ç†ŠC è¦æå‰é¢„ä¹ è‚²å„¿çŸ¥è¯†çš„è¯ï¼Œå»ºè®®çœ‹ä¸€äº›å°å·«å†™çš„ä¹¦ï¼Œå˜»å˜»\n",
      "çœŸå®æ˜¯éè°£è¨€ é¢„æµ‹æ˜¯éè°£è¨€ output=0.71\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    'å…´ä»å¿ä»Šå¤©æŠ¢å°å­©æ²¡æŠ¢èµ°ï¼ŒæŠŠå­©å­æ¯äº²æ…äº†ä¸€åˆ€ï¼Œçœ‹è§è¿™è½¦çš„æ³¨æ„äº†ï¼ŒçœŸäº‹ï¼Œè½¦ç‰Œå·è¾½HFM055ï¼ï¼ï¼ï¼ï¼èµ¶ç´§æ•£æ’­ï¼ éƒ½åˆ«å¸¦å­©å­å‡ºå»çè½¬æ‚ äº† å°¤å…¶åˆ«è®©è€äººè‡ªå·±å¸¦å­©å­å‡ºå» å¤ªå±é™©äº† æ³¨æ„äº†ï¼ï¼ï¼ï¼è¾½HFM055åŒ—äº¬ç°ä»£æœ—åŠ¨ï¼Œåœ¨å„å­¦æ ¡é—¨å£æŠ¢å°å­©ï¼ï¼ï¼110å·²ç» è¯å®ï¼ï¼å…¨å¸‚é€šç¼‰ï¼ï¼',\n",
    "    'é‡åº†çœŸå®æ–°é—»:2016å¹´6æœˆ1æ—¥åœ¨é‡åº†æ¢å¹³å¿è¢é©¿é•‡å‘ç”Ÿä¸€èµ·æŠ¢å„¿ç«¥äº‹ä»¶ï¼Œåšæ¡ˆäººä¸‰ä¸ªä¸­å¹´ç”·äººï¼Œåœ¨ä¸‰ä¸­å­¦æ ¡åˆ°é•‡è¡—ä¸Šçš„ä¸€æ¡å°è·¯ä¸Šï¼ŒæŠŠå°å­©ç›´æ¥å¼„æ™•(å„¿ç«¥æ˜¯è¢é©¿æ–°å¹¼å„¿å›­ä¸­ç­çš„ä¸€åå­¦ç”Ÿ)ï¼Œæ­£å‡†å¤‡å¸¦èµ°æ—¶è¢«å®¶é•¿åŠæ—¶å‘ç°ç”¨æ£’å­èµ¶èµ°äº†åšæ¡ˆäººï¼Œæ•…æ­¤è·æ•‘ï¼è¯·å„ä½åŒèƒä»¬ä»¥æ­¤å¼•èµ·éå¸¸é‡è§†ï¼Œå¸Œæœ›å¤§å®¶æœ‰çˆ±å¿ƒçš„äººä¼ é€’ä¸‹',\n",
    "    '@å°¾ç†ŠC è¦æå‰é¢„ä¹ è‚²å„¿çŸ¥è¯†çš„è¯ï¼Œå»ºè®®çœ‹ä¸€äº›å°å·«å†™çš„ä¹¦ï¼Œå˜»å˜»',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_LSTM(test_list[i],test_label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================model1========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rumor_GRU(text,label):\n",
    "    print(text)\n",
    "    # å»æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # åˆ†è¯\n",
    "    cut = seg.cut(text)\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # é¢„æµ‹\n",
    "    dic={0:'è°£è¨€',1:'éè°£è¨€'}\n",
    "    result = model1.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯éè°£è¨€','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯è°£è¨€','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…´ä»å¿ä»Šå¤©æŠ¢å°å­©æ²¡æŠ¢èµ°ï¼ŒæŠŠå­©å­æ¯äº²æ…äº†ä¸€åˆ€ï¼Œçœ‹è§è¿™è½¦çš„æ³¨æ„äº†ï¼ŒçœŸäº‹ï¼Œè½¦ç‰Œå·è¾½HFM055ï¼ï¼ï¼ï¼ï¼èµ¶ç´§æ•£æ’­ï¼ éƒ½åˆ«å¸¦å­©å­å‡ºå»çè½¬æ‚ äº† å°¤å…¶åˆ«è®©è€äººè‡ªå·±å¸¦å­©å­å‡ºå» å¤ªå±é™©äº† æ³¨æ„äº†ï¼ï¼ï¼ï¼è¾½HFM055åŒ—äº¬ç°ä»£æœ—åŠ¨ï¼Œåœ¨å„å­¦æ ¡é—¨å£æŠ¢å°å­©ï¼ï¼ï¼110å·²ç» è¯å®ï¼ï¼å…¨å¸‚é€šç¼‰ï¼ï¼\n",
      "çœŸå®æ˜¯è°£è¨€ é¢„æµ‹æ˜¯è°£è¨€ output=0.10\n",
      "---------------------------------------------\n",
      "é‡åº†çœŸå®æ–°é—»:2016å¹´6æœˆ1æ—¥åœ¨é‡åº†æ¢å¹³å¿è¢é©¿é•‡å‘ç”Ÿä¸€èµ·æŠ¢å„¿ç«¥äº‹ä»¶ï¼Œåšæ¡ˆäººä¸‰ä¸ªä¸­å¹´ç”·äººï¼Œåœ¨ä¸‰ä¸­å­¦æ ¡åˆ°é•‡è¡—ä¸Šçš„ä¸€æ¡å°è·¯ä¸Šï¼ŒæŠŠå°å­©ç›´æ¥å¼„æ™•(å„¿ç«¥æ˜¯è¢é©¿æ–°å¹¼å„¿å›­ä¸­ç­çš„ä¸€åå­¦ç”Ÿ)ï¼Œæ­£å‡†å¤‡å¸¦èµ°æ—¶è¢«å®¶é•¿åŠæ—¶å‘ç°ç”¨æ£’å­èµ¶èµ°äº†åšæ¡ˆäººï¼Œæ•…æ­¤è·æ•‘ï¼è¯·å„ä½åŒèƒä»¬ä»¥æ­¤å¼•èµ·éå¸¸é‡è§†ï¼Œå¸Œæœ›å¤§å®¶æœ‰çˆ±å¿ƒçš„äººä¼ é€’ä¸‹\n",
      "çœŸå®æ˜¯è°£è¨€ é¢„æµ‹æ˜¯è°£è¨€ output=0.11\n",
      "---------------------------------------------\n",
      "@å°¾ç†ŠC è¦æå‰é¢„ä¹ è‚²å„¿çŸ¥è¯†çš„è¯ï¼Œå»ºè®®çœ‹ä¸€äº›å°å·«å†™çš„ä¹¦ï¼Œå˜»å˜»\n",
      "çœŸå®æ˜¯éè°£è¨€ é¢„æµ‹æ˜¯éè°£è¨€ output=0.61\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    'å…´ä»å¿ä»Šå¤©æŠ¢å°å­©æ²¡æŠ¢èµ°ï¼ŒæŠŠå­©å­æ¯äº²æ…äº†ä¸€åˆ€ï¼Œçœ‹è§è¿™è½¦çš„æ³¨æ„äº†ï¼ŒçœŸäº‹ï¼Œè½¦ç‰Œå·è¾½HFM055ï¼ï¼ï¼ï¼ï¼èµ¶ç´§æ•£æ’­ï¼ éƒ½åˆ«å¸¦å­©å­å‡ºå»çè½¬æ‚ äº† å°¤å…¶åˆ«è®©è€äººè‡ªå·±å¸¦å­©å­å‡ºå» å¤ªå±é™©äº† æ³¨æ„äº†ï¼ï¼ï¼ï¼è¾½HFM055åŒ—äº¬ç°ä»£æœ—åŠ¨ï¼Œåœ¨å„å­¦æ ¡é—¨å£æŠ¢å°å­©ï¼ï¼ï¼110å·²ç» è¯å®ï¼ï¼å…¨å¸‚é€šç¼‰ï¼ï¼',\n",
    "    'é‡åº†çœŸå®æ–°é—»:2016å¹´6æœˆ1æ—¥åœ¨é‡åº†æ¢å¹³å¿è¢é©¿é•‡å‘ç”Ÿä¸€èµ·æŠ¢å„¿ç«¥äº‹ä»¶ï¼Œåšæ¡ˆäººä¸‰ä¸ªä¸­å¹´ç”·äººï¼Œåœ¨ä¸‰ä¸­å­¦æ ¡åˆ°é•‡è¡—ä¸Šçš„ä¸€æ¡å°è·¯ä¸Šï¼ŒæŠŠå°å­©ç›´æ¥å¼„æ™•(å„¿ç«¥æ˜¯è¢é©¿æ–°å¹¼å„¿å›­ä¸­ç­çš„ä¸€åå­¦ç”Ÿ)ï¼Œæ­£å‡†å¤‡å¸¦èµ°æ—¶è¢«å®¶é•¿åŠæ—¶å‘ç°ç”¨æ£’å­èµ¶èµ°äº†åšæ¡ˆäººï¼Œæ•…æ­¤è·æ•‘ï¼è¯·å„ä½åŒèƒä»¬ä»¥æ­¤å¼•èµ·éå¸¸é‡è§†ï¼Œå¸Œæœ›å¤§å®¶æœ‰çˆ±å¿ƒçš„äººä¼ é€’ä¸‹',\n",
    "    '@å°¾ç†ŠC è¦æå‰é¢„ä¹ è‚²å„¿çŸ¥è¯†çš„è¯ï¼Œå»ºè®®çœ‹ä¸€äº›å°å·«å†™çš„ä¹¦ï¼Œå˜»å˜»',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_GRU(test_list[i],test_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.1]",
   "language": "python",
   "name": "conda-env-TF2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
